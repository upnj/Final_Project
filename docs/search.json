[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Classification Tree and Random Forest Modeling",
    "section": "",
    "text": "In the EDA, we explored a dataset containing information on individuals’ health status, lifestyle behaviors, and demographic characteristics. The goal was to understand factors associated with diabetes prevalence. We found significant relationships between diabetes and age, education, income, BMI, physical activity, and other health conditions like high blood pressure and high cholesterol. Now, we will use this data to build predictive models for diabetes status. We will focus on two types of models: classification trees and random forests. These models will allow us to predict whether an individual has diabetes based on their characteristics.\n\n\nTo ensure a robust evaluation of our models, we will split the data into a training set (70% of the data) and a test set (30% of the data). The training set will be used to build and tune the models, while the test set will be used for final model evaluation.\n\n# Loading packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(httr)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(dplyr)\nlibrary(tidycensus)\nlibrary(lubridate)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(quantreg)\n\nLoading required package: SparseM\n\nlibrary(rsample)\n\nWarning: package 'rsample' was built under R version 4.4.2\n\nlibrary(parsnip)\n\nWarning: package 'parsnip' was built under R version 4.4.2\n\nlibrary(recipes)\n\nWarning: package 'recipes' was built under R version 4.4.2\n\n\n\nAttaching package: 'recipes'\n\nThe following object is masked from 'package:stringr':\n\n    fixed\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ tune         1.2.1\n✔ dials        1.3.0     ✔ workflows    1.1.4\n✔ infer        1.0.7     ✔ workflowsets 1.1.0\n✔ modeldata    1.4.0     ✔ yardstick    1.3.1\n\n\nWarning: package 'dials' was built under R version 4.4.2\n\n\nWarning: package 'infer' was built under R version 4.4.2\n\n\nWarning: package 'modeldata' was built under R version 4.4.2\n\n\nWarning: package 'tune' was built under R version 4.4.2\n\n\nWarning: package 'workflows' was built under R version 4.4.2\n\n\nWarning: package 'workflowsets' was built under R version 4.4.2\n\n\nWarning: package 'yardstick' was built under R version 4.4.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()   masks purrr::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ recipes::fixed()    masks stringr::fixed()\n✖ jsonlite::flatten() masks purrr::flatten()\n✖ dplyr::lag()        masks stats::lag()\n✖ yardstick::spec()   masks readr::spec()\n✖ recipes::step()     masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(rpart.plot)\n\nWarning: package 'rpart.plot' was built under R version 4.4.2\n\n\nLoading required package: rpart\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.2\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\ndiabetes_data &lt;- read.csv (\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\ndiabetes_data$Diabetes_binary &lt;- factor(diabetes_data$Diabetes_binary)\n# Set seed for reproducibility\nset.seed(123)\n\n# Split data into training (70%) and test (30%) sets\nsplit &lt;- initial_split(diabetes_data, prop = 0.7, strata = Diabetes_binary)\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\n\nstr(diabetes_data)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : int  1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : int  1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : int  1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : int  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : int  1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : int  0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: int  0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : int  0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : int  0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : int  1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : int  1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : int  0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : int  5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : int  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : int  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : int  1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : int  0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : int  9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : int  4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : int  3 1 8 6 4 8 7 4 1 3 ...\n\nstr(test_data)\n\n'data.frame':   76105 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 2 1 1 2 2 ...\n $ HighBP              : int  0 1 1 1 1 1 1 0 1 1 ...\n $ HighChol            : int  0 0 1 1 1 1 1 0 0 1 ...\n $ CholCheck           : int  0 1 1 1 1 1 1 0 1 1 ...\n $ BMI                 : int  25 27 24 25 30 28 21 23 27 37 ...\n $ Smoker              : int  1 0 0 1 1 0 0 0 0 1 ...\n $ Stroke              : int  0 0 0 0 0 0 0 0 0 1 ...\n $ HeartDiseaseorAttack: int  0 0 0 0 1 0 0 0 0 1 ...\n $ PhysActivity        : int  1 1 1 1 0 0 1 0 1 0 ...\n $ Fruits              : int  0 1 1 1 1 0 1 0 1 0 ...\n $ Veggies             : int  0 1 1 1 1 1 1 1 1 1 ...\n $ HvyAlcoholConsump   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : int  0 1 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : int  1 0 0 0 0 0 0 0 0 0 ...\n $ GenHlth             : int  3 2 2 2 5 4 3 2 1 5 ...\n $ MentHlth            : int  0 0 3 0 30 0 0 15 0 0 ...\n $ PhysHlth            : int  0 0 0 2 30 0 0 0 0 0 ...\n $ DiffWalk            : int  0 0 0 0 1 1 0 0 0 1 ...\n $ Sex                 : int  0 0 0 1 0 0 0 0 0 1 ...\n $ Age                 : int  7 11 11 10 9 11 10 2 13 10 ...\n $ Education           : int  6 3 5 6 5 4 4 6 5 6 ...\n $ Income              : int  1 6 4 8 1 6 3 7 4 5 ...\n\nstr(train_data)\n\n'data.frame':   177575 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HighBP              : int  1 1 1 1 0 1 0 0 1 0 ...\n $ HighChol            : int  1 1 0 1 0 1 0 1 0 1 ...\n $ CholCheck           : int  1 1 1 1 1 1 1 1 1 1 ...\n $ BMI                 : int  40 28 30 25 24 34 26 33 33 28 ...\n $ Smoker              : int  1 0 1 1 0 1 1 1 0 0 ...\n $ Stroke              : int  0 0 0 0 0 0 0 1 0 0 ...\n $ HeartDiseaseorAttack: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PhysActivity        : int  0 0 0 1 0 0 0 1 1 0 ...\n $ Fruits              : int  0 1 0 0 0 1 0 0 0 0 ...\n $ Veggies             : int  1 0 0 1 1 1 1 1 0 0 ...\n $ HvyAlcoholConsump   : int  0 0 0 0 0 0 0 0 0 1 ...\n $ AnyHealthcare       : int  1 1 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : int  0 1 0 0 0 0 0 1 0 0 ...\n $ GenHlth             : int  5 5 3 3 2 3 3 4 2 2 ...\n $ MentHlth            : int  18 30 0 0 0 0 0 30 5 10 ...\n $ PhysHlth            : int  15 30 14 0 0 30 15 28 0 0 ...\n $ DiffWalk            : int  1 1 0 1 0 1 0 0 0 0 ...\n $ Sex                 : int  0 0 0 0 1 0 0 0 0 1 ...\n $ Age                 : int  9 9 9 11 8 10 7 4 6 4 ...\n $ Education           : int  4 4 6 4 4 5 5 6 6 6 ...\n $ Income              : int  3 8 7 4 3 1 7 2 8 8 ...\n\n\n\n\n\nClassification trees are very similar to regression trees except, of course, our response is a categorical variable. This means that we don’t use the same loss functions nor metrics, but we still split the predictor space up into regions. We then can make our prediction based on which bin an observation ends up in. Most often, we use the most prevalent class in a bin as our prediction.\nA classification tree is a model that predicts a categorical outcome (in this case, diabetes status) by recursively partitioning the data based on the predictor variables. The tree starts with all data in a single node and then finds the best predictor and split point to divide the data into two more homogeneous subgroups. This process is repeated for each subgroup until a stopping criterion is met. The complexity of a classification tree is controlled by the complexity parameter (cp). A smaller cp results in a larger, more complex tree that may overfit the data. A larger cp yields a smaller, simpler tree that may underfit. We will tune the cp parameter using 5-fold cross-validation to find the optimal balance.\n\n# Define the model\ntree_model &lt;- decision_tree(\n  mode = \"classification\",\n  cost_complexity = tune(),\n  tree_depth = tune()\n) %&gt;%\n  set_mode(\"classification\")%&gt;% set_engine(\"rpart\")\n\n\n# Define the recipe\ntree_recipe &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + PhysActivity + Sex  + GenHlth +                         HvyAlcoholConsump + MentHlth + PhysHlth , \n                      data = train_data) %&gt;%\n  step_dummy(all_nominal_predictors())\n\nprint(tree_recipe)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 8\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: all_nominal_predictors()\n\n# Define the workflow\ntree_workflow &lt;- workflow() %&gt;%\n  add_model(tree_model) %&gt;%\n  add_recipe(tree_recipe)\n\nprint(tree_workflow)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\n# Define the cross-validation \n\ntree_cv &lt;- vfold_cv(train_data, v = 5, strata = Diabetes_binary)\nprint(tree_cv)\n\n#  5-fold cross-validation using stratification \n# A tibble: 5 × 2\n  splits                 id   \n  &lt;list&gt;                 &lt;chr&gt;\n1 &lt;split [142059/35516]&gt; Fold1\n2 &lt;split [142059/35516]&gt; Fold2\n3 &lt;split [142060/35515]&gt; Fold3\n4 &lt;split [142061/35514]&gt; Fold4\n5 &lt;split [142061/35514]&gt; Fold5\n\n# Define the parameter grid\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n print(tree_grid)  \n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             &lt;dbl&gt;      &lt;int&gt;\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# ℹ 15 more rows\n\n# Tune the model                          \ntree_res &lt;- tune_grid(\n  tree_workflow,\n  resamples = tree_cv,\n  grid = tree_grid,\n  metrics = metric_set(mn_log_loss)\n)\n\n\ntree_metrics &lt;- collect_metrics(tree_res)\nprint(tree_metrics)\n\n# A tibble: 25 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 2    0.0000000178          1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 3    0.00000316            1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 4    0.000562              1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 5    0.1                   1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 6    0.0000000001          4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 7    0.0000000178          4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 8    0.00000316            4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 9    0.000562              4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n10    0.1                   4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n# ℹ 15 more rows\n\nprint(tree_res)\n\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 4\n  splits                 id    .metrics          .notes          \n  &lt;list&gt;                 &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [142059/35516]&gt; Fold1 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [142059/35516]&gt; Fold2 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [142060/35515]&gt; Fold3 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [142061/35514]&gt; Fold4 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [142061/35514]&gt; Fold5 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\nprint(tree_metrics)\n\n# A tibble: 25 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 2    0.0000000178          1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 3    0.00000316            1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 4    0.000562              1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 5    0.1                   1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 6    0.0000000001          4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 7    0.0000000178          4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 8    0.00000316            4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 9    0.000562              4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n10    0.1                   4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n# ℹ 15 more rows\n\n# Select best model\nbest_tree &lt;- tree_res %&gt;%\n  select_best(metric = \"mn_log_loss\")\n\nprint(best_tree)\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001         11 Preprocessor1_Model16\n\n# Finalize workflow with best model  \nfinal_tree_workflow &lt;- tree_workflow %&gt;%\n  finalize_workflow(best_tree)\n\n# Fit the final model to training data\nfinal_tree_fit &lt;- final_tree_workflow %&gt;%\n  fit(data = train_data)\n\n# Evaluate the model on the test set\nfinal_tree_results &lt;- final_tree_workflow %&gt;%\n  last_fit(split)\n\n# Extract the log loss metric\nfinal_tree_log_loss &lt;- final_tree_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"mn_log_loss\") %&gt;%\n  pull(.estimate)\n\n\n# Extract the trained model object\ntrained_tree &lt;- final_tree_fit %&gt;%\n  extract_fit_engine() \n  \n\n# Create the plot\n\nrpart.plot(trained_tree, roundint = FALSE, extra = 106, cex=0.08)\n\nWarning: labs do not fit even at cex 0.15, there may be some overplotting"
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Classification Tree and Random Forest Modeling",
    "section": "",
    "text": "In the EDA, we explored a dataset containing information on individuals’ health status, lifestyle behaviors, and demographic characteristics. The goal was to understand factors associated with diabetes prevalence. We found significant relationships between diabetes and age, education, income, BMI, physical activity, and other health conditions like high blood pressure and high cholesterol. Now, we will use this data to build predictive models for diabetes status. We will focus on two types of models: classification trees and random forests. These models will allow us to predict whether an individual has diabetes based on their characteristics.\n\n\nTo ensure a robust evaluation of our models, we will split the data into a training set (70% of the data) and a test set (30% of the data). The training set will be used to build and tune the models, while the test set will be used for final model evaluation.\n\n# Loading packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(httr)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(dplyr)\nlibrary(tidycensus)\nlibrary(lubridate)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(quantreg)\n\nLoading required package: SparseM\n\nlibrary(rsample)\n\nWarning: package 'rsample' was built under R version 4.4.2\n\nlibrary(parsnip)\n\nWarning: package 'parsnip' was built under R version 4.4.2\n\nlibrary(recipes)\n\nWarning: package 'recipes' was built under R version 4.4.2\n\n\n\nAttaching package: 'recipes'\n\nThe following object is masked from 'package:stringr':\n\n    fixed\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ tune         1.2.1\n✔ dials        1.3.0     ✔ workflows    1.1.4\n✔ infer        1.0.7     ✔ workflowsets 1.1.0\n✔ modeldata    1.4.0     ✔ yardstick    1.3.1\n\n\nWarning: package 'dials' was built under R version 4.4.2\n\n\nWarning: package 'infer' was built under R version 4.4.2\n\n\nWarning: package 'modeldata' was built under R version 4.4.2\n\n\nWarning: package 'tune' was built under R version 4.4.2\n\n\nWarning: package 'workflows' was built under R version 4.4.2\n\n\nWarning: package 'workflowsets' was built under R version 4.4.2\n\n\nWarning: package 'yardstick' was built under R version 4.4.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()   masks purrr::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ recipes::fixed()    masks stringr::fixed()\n✖ jsonlite::flatten() masks purrr::flatten()\n✖ dplyr::lag()        masks stats::lag()\n✖ yardstick::spec()   masks readr::spec()\n✖ recipes::step()     masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(rpart.plot)\n\nWarning: package 'rpart.plot' was built under R version 4.4.2\n\n\nLoading required package: rpart\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.2\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\ndiabetes_data &lt;- read.csv (\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\ndiabetes_data$Diabetes_binary &lt;- factor(diabetes_data$Diabetes_binary)\n# Set seed for reproducibility\nset.seed(123)\n\n# Split data into training (70%) and test (30%) sets\nsplit &lt;- initial_split(diabetes_data, prop = 0.7, strata = Diabetes_binary)\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\n\nstr(diabetes_data)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : int  1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : int  1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : int  1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : int  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : int  1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : int  0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: int  0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : int  0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : int  0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : int  1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : int  1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : int  0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : int  5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : int  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : int  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : int  1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : int  0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : int  9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : int  4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : int  3 1 8 6 4 8 7 4 1 3 ...\n\nstr(test_data)\n\n'data.frame':   76105 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 2 1 1 2 2 ...\n $ HighBP              : int  0 1 1 1 1 1 1 0 1 1 ...\n $ HighChol            : int  0 0 1 1 1 1 1 0 0 1 ...\n $ CholCheck           : int  0 1 1 1 1 1 1 0 1 1 ...\n $ BMI                 : int  25 27 24 25 30 28 21 23 27 37 ...\n $ Smoker              : int  1 0 0 1 1 0 0 0 0 1 ...\n $ Stroke              : int  0 0 0 0 0 0 0 0 0 1 ...\n $ HeartDiseaseorAttack: int  0 0 0 0 1 0 0 0 0 1 ...\n $ PhysActivity        : int  1 1 1 1 0 0 1 0 1 0 ...\n $ Fruits              : int  0 1 1 1 1 0 1 0 1 0 ...\n $ Veggies             : int  0 1 1 1 1 1 1 1 1 1 ...\n $ HvyAlcoholConsump   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : int  0 1 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : int  1 0 0 0 0 0 0 0 0 0 ...\n $ GenHlth             : int  3 2 2 2 5 4 3 2 1 5 ...\n $ MentHlth            : int  0 0 3 0 30 0 0 15 0 0 ...\n $ PhysHlth            : int  0 0 0 2 30 0 0 0 0 0 ...\n $ DiffWalk            : int  0 0 0 0 1 1 0 0 0 1 ...\n $ Sex                 : int  0 0 0 1 0 0 0 0 0 1 ...\n $ Age                 : int  7 11 11 10 9 11 10 2 13 10 ...\n $ Education           : int  6 3 5 6 5 4 4 6 5 6 ...\n $ Income              : int  1 6 4 8 1 6 3 7 4 5 ...\n\nstr(train_data)\n\n'data.frame':   177575 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HighBP              : int  1 1 1 1 0 1 0 0 1 0 ...\n $ HighChol            : int  1 1 0 1 0 1 0 1 0 1 ...\n $ CholCheck           : int  1 1 1 1 1 1 1 1 1 1 ...\n $ BMI                 : int  40 28 30 25 24 34 26 33 33 28 ...\n $ Smoker              : int  1 0 1 1 0 1 1 1 0 0 ...\n $ Stroke              : int  0 0 0 0 0 0 0 1 0 0 ...\n $ HeartDiseaseorAttack: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PhysActivity        : int  0 0 0 1 0 0 0 1 1 0 ...\n $ Fruits              : int  0 1 0 0 0 1 0 0 0 0 ...\n $ Veggies             : int  1 0 0 1 1 1 1 1 0 0 ...\n $ HvyAlcoholConsump   : int  0 0 0 0 0 0 0 0 0 1 ...\n $ AnyHealthcare       : int  1 1 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : int  0 1 0 0 0 0 0 1 0 0 ...\n $ GenHlth             : int  5 5 3 3 2 3 3 4 2 2 ...\n $ MentHlth            : int  18 30 0 0 0 0 0 30 5 10 ...\n $ PhysHlth            : int  15 30 14 0 0 30 15 28 0 0 ...\n $ DiffWalk            : int  1 1 0 1 0 1 0 0 0 0 ...\n $ Sex                 : int  0 0 0 0 1 0 0 0 0 1 ...\n $ Age                 : int  9 9 9 11 8 10 7 4 6 4 ...\n $ Education           : int  4 4 6 4 4 5 5 6 6 6 ...\n $ Income              : int  3 8 7 4 3 1 7 2 8 8 ...\n\n\n\n\n\nClassification trees are very similar to regression trees except, of course, our response is a categorical variable. This means that we don’t use the same loss functions nor metrics, but we still split the predictor space up into regions. We then can make our prediction based on which bin an observation ends up in. Most often, we use the most prevalent class in a bin as our prediction.\nA classification tree is a model that predicts a categorical outcome (in this case, diabetes status) by recursively partitioning the data based on the predictor variables. The tree starts with all data in a single node and then finds the best predictor and split point to divide the data into two more homogeneous subgroups. This process is repeated for each subgroup until a stopping criterion is met. The complexity of a classification tree is controlled by the complexity parameter (cp). A smaller cp results in a larger, more complex tree that may overfit the data. A larger cp yields a smaller, simpler tree that may underfit. We will tune the cp parameter using 5-fold cross-validation to find the optimal balance.\n\n# Define the model\ntree_model &lt;- decision_tree(\n  mode = \"classification\",\n  cost_complexity = tune(),\n  tree_depth = tune()\n) %&gt;%\n  set_mode(\"classification\")%&gt;% set_engine(\"rpart\")\n\n\n# Define the recipe\ntree_recipe &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + PhysActivity + Sex  + GenHlth +                         HvyAlcoholConsump + MentHlth + PhysHlth , \n                      data = train_data) %&gt;%\n  step_dummy(all_nominal_predictors())\n\nprint(tree_recipe)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 8\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: all_nominal_predictors()\n\n# Define the workflow\ntree_workflow &lt;- workflow() %&gt;%\n  add_model(tree_model) %&gt;%\n  add_recipe(tree_recipe)\n\nprint(tree_workflow)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\n# Define the cross-validation \n\ntree_cv &lt;- vfold_cv(train_data, v = 5, strata = Diabetes_binary)\nprint(tree_cv)\n\n#  5-fold cross-validation using stratification \n# A tibble: 5 × 2\n  splits                 id   \n  &lt;list&gt;                 &lt;chr&gt;\n1 &lt;split [142059/35516]&gt; Fold1\n2 &lt;split [142059/35516]&gt; Fold2\n3 &lt;split [142060/35515]&gt; Fold3\n4 &lt;split [142061/35514]&gt; Fold4\n5 &lt;split [142061/35514]&gt; Fold5\n\n# Define the parameter grid\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n print(tree_grid)  \n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             &lt;dbl&gt;      &lt;int&gt;\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# ℹ 15 more rows\n\n# Tune the model                          \ntree_res &lt;- tune_grid(\n  tree_workflow,\n  resamples = tree_cv,\n  grid = tree_grid,\n  metrics = metric_set(mn_log_loss)\n)\n\n\ntree_metrics &lt;- collect_metrics(tree_res)\nprint(tree_metrics)\n\n# A tibble: 25 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 2    0.0000000178          1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 3    0.00000316            1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 4    0.000562              1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 5    0.1                   1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 6    0.0000000001          4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 7    0.0000000178          4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 8    0.00000316            4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 9    0.000562              4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n10    0.1                   4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n# ℹ 15 more rows\n\nprint(tree_res)\n\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 4\n  splits                 id    .metrics          .notes          \n  &lt;list&gt;                 &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [142059/35516]&gt; Fold1 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [142059/35516]&gt; Fold2 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [142060/35515]&gt; Fold3 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [142061/35514]&gt; Fold4 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [142061/35514]&gt; Fold5 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\nprint(tree_metrics)\n\n# A tibble: 25 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 2    0.0000000178          1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 3    0.00000316            1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 4    0.000562              1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 5    0.1                   1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 6    0.0000000001          4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 7    0.0000000178          4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 8    0.00000316            4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 9    0.000562              4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n10    0.1                   4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n# ℹ 15 more rows\n\n# Select best model\nbest_tree &lt;- tree_res %&gt;%\n  select_best(metric = \"mn_log_loss\")\n\nprint(best_tree)\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001         11 Preprocessor1_Model16\n\n# Finalize workflow with best model  \nfinal_tree_workflow &lt;- tree_workflow %&gt;%\n  finalize_workflow(best_tree)\n\n# Fit the final model to training data\nfinal_tree_fit &lt;- final_tree_workflow %&gt;%\n  fit(data = train_data)\n\n# Evaluate the model on the test set\nfinal_tree_results &lt;- final_tree_workflow %&gt;%\n  last_fit(split)\n\n# Extract the log loss metric\nfinal_tree_log_loss &lt;- final_tree_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"mn_log_loss\") %&gt;%\n  pull(.estimate)\n\n\n# Extract the trained model object\ntrained_tree &lt;- final_tree_fit %&gt;%\n  extract_fit_engine() \n  \n\n# Create the plot\n\nrpart.plot(trained_tree, roundint = FALSE, extra = 106, cex=0.08)\n\nWarning: labs do not fit even at cex 0.15, there may be some overplotting"
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "Classification Tree and Random Forest Modeling",
    "section": "Random Forest:",
    "text": "Random Forest:\nA random forest is an ensemble learning method that builds upon the concept of decision trees. While a single decision tree splits the data based on the best available feature at each node, it can be prone to overfitting and high variance. Random forests address these limitations by:\n\nBootstrap Aggregating (Bagging):\nCreates multiple trees using different random samples of the training data Each tree gets a slightly different dataset, reducing variance Final prediction is made by majority voting (for classification)\n\n\nRandom Feature Selection:\nAt each split, only a random subset of features is considered This subset size is controlled by the ‘mtry’ parameter Helps reduce correlation between trees, improving model robustness\nThe key advantages of random forests over single decision trees include:\nReduced overfitting Lower variance while maintaining low bias Better generalization to new data Built-in feature importance measures Handles non-linear relationships well\nIn our implementation, we’re tuning the ‘mtry’ parameter, which controls how many features are considered at each split. We’re using 5-fold cross-validation to find the optimal value, trying values from 2 to 8 features. The model includes important predictors like:\nHighBP (High Blood Pressure) HighChol (High Cholesterol) BMI (Body Mass Index) Sex Age GenHlth (General Health) Physical Activity Alcohol Consumption Mental and Physical Health metrics\nA random forest is an ensemble of many classification trees. Each tree in the forest is built on a bootstrap sample of the training data, and at each split, only a random subset of predictors (mtry) is considered. This introduces randomness and diversity among the trees, which often leads to better predictive performance than a single tree. The final prediction is made by aggregating the predictions of all trees (majority vote for classification). We will tune the mtry parameter, which controls the number of predictors considered at each split, using 5-fold cross-validation.\n\n# Define the parameter grid  \nrf_grid &lt;- grid_regular(\n   mtry(range = c(2, 6)),\n  levels = 5\n)\n\n# Calculate class weights based on class proportions\nclass_weights &lt;- table(train_data$Diabetes_binary)\nweights &lt;- rev(class_weights/sum(class_weights))\n\n# Define the model with class weights\nrf_model &lt;- rand_forest(\n  mode = \"classification\",\n  mtry = tune(),\n  trees = 100\n) %&gt;%\n  set_engine(\"randomForest\", \n             classwt = weights)  # Directly specify weights\n\nprint(rf_model)\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 100\n\nEngine-Specific Arguments:\n  classwt = weights\n\nComputational engine: randomForest \n\n# Define the workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_model(rf_model) %&gt;%\n  add_recipe(tree_recipe)  # Reuse the recipe from classification tree\n\n\n# Tune the model\nset.seed(123)\nrf_res &lt;- tune_grid(\n  rf_workflow,\n  resamples = tree_cv,  # Reuse the CV splits from classification tree\n  grid = rf_grid,\n  metrics = metric_set(mn_log_loss)\n)\n\nWarning: package 'randomForest' was built under R version 4.4.2\n\nrf_plot &lt;-rf_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"mn_log_loss\") %&gt;%\n  ggplot(aes(x = mtry, y = mean)) +\n  geom_line() +\n  geom_point() +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err)) +\n  labs(title = \"Model Performance vs mtry\",\n       y = \"Mean Log Loss\",\n       x = \"Number of Variables Sampled (mtry)\")\n\nprint(rf_plot)\n\n\n\n\n\n\n\n# Select best model  \nbest_rf &lt;- rf_res %&gt;%\n  select_best(metric = \"mn_log_loss\")\n\nprint(\"Best parameters:\")\n\n[1] \"Best parameters:\"\n\nprint(best_rf)\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     6 Preprocessor1_Model5\n\n# Finalize workflow with best model\nfinal_rf &lt;- rf_workflow %&gt;%\n  finalize_workflow(best_rf)%&gt;%\n  fit(train_data)\n\n\n# Get variable importance\nimportance_plot &lt;- final_rf %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(num_features = 10)\n\nprint(importance_plot)\n\n\n\n\n\n\n\n\nThis variable importance plot from our random forest model reveals several key insights about predicting diabetes:\nMost Important Variables:\nGenHlth (General Health) is the most important predictor HighBP (High Blood Pressure) is a close second PhysHlth (Physical Health) and MentHlth (Mental Health) are also significant predictors\nModerately Important:\nHighChol (High Cholesterol) shows moderate importance Sex has lower but still notable importance\nLess Important Variables:\nPhysActivity (Physical Activity) HvyAlcoholConsump (Heavy Alcohol Consumption) show relatively low importance\nThe relative importance is measured by how much each variable helps reduce prediction error in the random forest model. Notably, both physical and mental health metrics play substantial roles in diabetes prediction, suggesting a holistic health approach is important for understanding diabetes risk.\n\n# Show tuning results\nrf_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"mn_log_loss\") %&gt;%  \n  arrange(mean)  # Note: For log loss, lower is better\n\n# A tibble: 5 × 7\n   mtry .metric     .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     6 mn_log_loss binary      3.20     5  0.0266 Preprocessor1_Model5\n2     5 mn_log_loss binary      3.20     5  0.0266 Preprocessor1_Model4\n3     4 mn_log_loss binary      3.24     5  0.0216 Preprocessor1_Model3\n4     3 mn_log_loss binary      3.38     5  0.0347 Preprocessor1_Model2\n5     2 mn_log_loss binary      3.55     5  0.0257 Preprocessor1_Model1\n\n# Make predictions on test set\n#test_predictions &lt;- predict(final_rf, test_data, type = \"prob\") %&gt;%\n#  bind_cols(test_data)\n\ntest_predictions &lt;- predict(final_rf, test_data, type = \"prob\")%&gt;%\n  bind_cols(test_data %&gt;% select(Diabetes_binary))\n#optimal_threshold &lt;- optimize_threshold(predictions, test_data$Diabetes_binary)\n\n# Calculate final log loss on test set\ntest_log_loss &lt;- mn_log_loss(\n  test_predictions,\n  truth = Diabetes_binary,\n  \n  .pred_1\n)\n\nprint(paste(\"Test set log loss:\", round(test_log_loss$.estimate, 4)))\n\n[1] \"Test set log loss: 26.8518\"\n\n\nThe log loss value = 26.8504 is too high this is due to imbalance dataset.\n\n\nFinal Model Selection:\nWe now have the best classification tree and random forest models based on their performance on the training set. To select the overall best model, we will evaluate both on the test set using log loss.\n\n# Make predictions on test set\n## Classification Tree\ntree_test_predictions &lt;- predict(final_tree_fit, test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data %&gt;% select(Diabetes_binary))\n\n## Random Forest\nrf_test_predictions &lt;- predict(final_rf, test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data %&gt;% select(Diabetes_binary)) \n\n# Combine predictions with actual values\n## Classification Tree\n\ntree_test_log_loss &lt;- mn_log_loss(\n  tree_test_predictions,\n  truth = Diabetes_binary,\n  .pred_1\n)\nprint(paste(\"Test set log loss for Classification Tree:\", round(tree_test_log_loss$.estimate, 4)))\n\n[1] \"Test set log loss for Classification Tree: 2.2937\"\n\n## Random Forest\nrf_test_log_loss &lt;- mn_log_loss(\n  rf_test_predictions,\n  truth = Diabetes_binary,\n  .pred_1\n)\nprint(paste(\"Test set log loss for Random Forest:\", round(rf_test_log_loss$.estimate, 4)))\n\n[1] \"Test set log loss for Random Forest: 26.8518\"\n\n# Compare models\n# Create a data frame with model names and log loss values\ncomparison_df &lt;- data.frame(\n  Model = c(\"Classification Tree\", \"Random Forest\"),\n  Log_Loss = c(tree_test_log_loss$.estimate, rf_test_log_loss$.estimate)\n)\n\n# Print the comparison data frame\nprint(comparison_df)\n\n                Model  Log_Loss\n1 Classification Tree  2.293652\n2       Random Forest 26.851783\n\n# Determine the best model based on log loss\nbest_model &lt;- comparison_df$Model[which.min(comparison_df$Log_Loss)]\ncat(\"The best model based on log loss is:\", best_model, \"\\n\")\n\nThe best model based on log loss is: Classification Tree \n\n# Optionally, you can create a bar plot to visualize the comparison\nlibrary(ggplot2)\n\nggplot(comparison_df, aes(x = Model, y = Log_Loss, fill = Model)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Model Comparison\", x = \"Model\", y = \"Log Loss\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nWhat risk factors are most predictive of diabetes risk?\nFrom the variable importance plot from the Random Forest model and the structure of the Classification Tree model.\nMost predictive risk factors for diabetes: According to the variable importance plot from the Random Forest model, the top predictive factors for diabetes risk are:\nHigh blood pressure (HighBP) High cholesterol (HighChol) Physical health (PhysHlth) Physical activity (PhysActivity) Sex General health (GenHlth)\nThese factors have the highest importance scores, indicating they contribute the most to the model’s predictions.\n\n\nCan we use a subset of the risk factors to accurately predict whether an individual has diabetes?\nUsing a subset of risk factors for accurate diabetes prediction: Looking at the structure of the Classification Tree model, we can see that it uses a subset of the available risk factors to make predictions. The tree splits on the following variables:\nHighBP HighChol GenHlth PhysActivity Sex\nThe Classification Tree model achieved a lower log loss (2.293652) compared to the Random Forest model (26.850450), indicating that this subset of risk factors can accurately predict diabetes risk. The tree structure suggests that having high blood pressure (HighBP = 1) and high cholesterol (HighChol = 1) are strong indicators of diabetes risk. The model further splits on general health status (GenHlth), physical activity (PhysActivity), and sex to refine the predictions. The fact that the Classification Tree model performs well using only a subset of the risk factors highlights the importance of these specific factors in predicting diabetes risk. This suggests that a simplified model focusing on key risk factors like high blood pressure, high cholesterol, general health, physical activity, and sex can provide accurate predictions.\nThe model with the lower log loss on the test set is the overall winner. Based on the results, the classification tree outperforms the random forest model in predicting diabetes status for this dataset. The evaluation metric used for comparison was log loss, which measures the dissimilarity between the predicted probabilities and the actual target values. A lower log loss indicates better predictive performance. The Classification Tree model achieved a log loss of 2.293652 on the test set, while the Random Forest model had a significantly higher log loss of 26.850450. This substantial difference suggests that the Classification Tree model is more accurate in its predictions and better captures the underlying patterns in the data. The best model was determined by selecting the model with the lowest log loss value. In this case, the Classification Tree model clearly outperformed the Random Forest model. It’s important to note that while log loss is a useful metric for evaluating probabilistic predictions, it’s always a good practice to consider multiple evaluation metrics and assess the model’s performance from different perspectives. Additionally, the specific dataset, feature selection, and preprocessing steps can impact model performance. Based on the provided log loss values, the Classification Tree model is the recommended choice for this particular diabetes prediction task. It demonstrates better predictive accuracy and is likely to provide more reliable predictions on unseen data\nIn summary, based on the variable importance from the Random Forest model and the structure of the Classification Tree model, the most predictive risk factors for diabetes are high blood pressure, high cholesterol, physical health, physical activity, sex, and general health. The Classification Tree model demonstrates that using a subset of these risk factors can accurately predict diabetes risk, with high blood pressure and high cholesterol being particularly strong indicators. This information can guide targeted risk assessment and preventive strategies in clinical practice.\nIn conclusion, we have built and compared two machine learning models for predicting diabetes status. The Classification Tree model emerged as the best model. This model could be useful for identifying individuals at high risk of diabetes based on their health profile and demographic characteristics."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Final Project: Diabetic Dataset Analysis",
    "section": "",
    "text": "The Behavioral Risk Factor Surveillance System (BRFSS) is a health-related telephone survey that is collected annually by the CDC. Each year, the survey collects responses from over 400,000 Americans on health-related risk behaviors, chronic health conditions, and the use of preventative services. diabetes _ binary _ health _ indicators _ BRFSS2015.csv is a clean data set of 253,680 survey responses to the CDC’s BRFSS2015. The target variable Diabetes_binary has 2 classes. 0 is for no diabetes, and 1 is for pre-diabetes or diabetes. This data set has 21 feature variables and is not balanced.The ultimate goal is to build a predictive model that can identify individuals at risk of developing diabetes based on the available data.\n\n\nWe perform the exploratory data analysis (EDA) to clean the data and understand the various variables. Check the missing values and investigate if the data is balanced or not. We can run basic summary statistics to get the feel for the data. The purpose of this EDA is to investigate the relationships between various health indicators and the presence of diabetes in the population.\nIn this data set I will be focusing on binary variables (diabetes, sex etc) which have “Yes/No” or male/female values.The target variable is “Diabetes_binary”. The predictor variables are Age, Income, education etc. The meaning full factor levels should be created for ordinal variables (General Health, Education, Age and Income).\n\n# Loading packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(httr)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(dplyr)\nlibrary(tidycensus)\nlibrary(lubridate)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(quantreg)\n\nLoading required package: SparseM\n\n#Read in the data\ndiabetes_data &lt;- read.csv (\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n# Inspect the data\nstr(diabetes_data)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : int  0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP              : int  1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : int  1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : int  1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : int  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : int  1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : int  0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: int  0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : int  0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : int  0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : int  1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : int  1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : int  0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : int  5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : int  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : int  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : int  1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : int  0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : int  9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : int  4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : int  3 1 8 6 4 8 7 4 1 3 ...\n\nsummary(diabetes_data)\n\n Diabetes_binary      HighBP         HighChol        CholCheck     \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :0.0000   Median :0.000   Median :0.0000   Median :1.0000  \n Mean   :0.1393   Mean   :0.429   Mean   :0.4241   Mean   :0.9627  \n 3rd Qu.:0.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n      BMI            Smoker           Stroke        HeartDiseaseorAttack\n Min.   :12.00   Min.   :0.0000   Min.   :0.00000   Min.   :0.00000     \n 1st Qu.:24.00   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000     \n Median :27.00   Median :0.0000   Median :0.00000   Median :0.00000     \n Mean   :28.38   Mean   :0.4432   Mean   :0.04057   Mean   :0.09419     \n 3rd Qu.:31.00   3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:0.00000     \n Max.   :98.00   Max.   :1.0000   Max.   :1.00000   Max.   :1.00000     \n  PhysActivity        Fruits          Veggies       HvyAlcoholConsump\n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   \n Median :1.0000   Median :1.0000   Median :1.0000   Median :0.0000   \n Mean   :0.7565   Mean   :0.6343   Mean   :0.8114   Mean   :0.0562   \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   \n AnyHealthcare     NoDocbcCost         GenHlth         MentHlth     \n Min.   :0.0000   Min.   :0.00000   Min.   :1.000   Min.   : 0.000  \n 1st Qu.:1.0000   1st Qu.:0.00000   1st Qu.:2.000   1st Qu.: 0.000  \n Median :1.0000   Median :0.00000   Median :2.000   Median : 0.000  \n Mean   :0.9511   Mean   :0.08418   Mean   :2.511   Mean   : 3.185  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:3.000   3rd Qu.: 2.000  \n Max.   :1.0000   Max.   :1.00000   Max.   :5.000   Max.   :30.000  \n    PhysHlth         DiffWalk           Sex              Age        \n Min.   : 0.000   Min.   :0.0000   Min.   :0.0000   Min.   : 1.000  \n 1st Qu.: 0.000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 6.000  \n Median : 0.000   Median :0.0000   Median :0.0000   Median : 8.000  \n Mean   : 4.242   Mean   :0.1682   Mean   :0.4403   Mean   : 8.032  \n 3rd Qu.: 3.000   3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:10.000  \n Max.   :30.000   Max.   :1.0000   Max.   :1.0000   Max.   :13.000  \n   Education        Income     \n Min.   :1.00   Min.   :1.000  \n 1st Qu.:4.00   1st Qu.:5.000  \n Median :5.00   Median :7.000  \n Mean   :5.05   Mean   :6.054  \n 3rd Qu.:6.00   3rd Qu.:8.000  \n Max.   :6.00   Max.   :8.000  \n\n# Check for missing values\ncolSums(is.na(diabetes_data))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\nThere are no missing values in the data. First, let’s take a look at the distribution of the target variable, “Diabetes_binary”:\n\ntable(diabetes_data$Diabetes_binary)\n\n\n     0      1 \n218334  35346 \n\nprop.table(table(diabetes_data$Diabetes_binary))\n\n\n       0        1 \n0.860667 0.139333 \n\n\nThis shows that 14% of the individuals in the dataset have diabetes. Next, let’s explore the relationships between the predictor variables and the presence of diabetes. We can start by looking at the prevalence of diabetes across different categories of the predictor variables:\n\n # Convert categorical variables to factors\ncols &lt;- c(\"HighBP\", \"HighChol\", \"CholCheck\", \"Smoker\", \"Stroke\", \"HeartDiseaseorAttack\", \n          \"PhysActivity\", \"Fruits\", \"Veggies\", \"HvyAlcoholConsump\", \"AnyHealthcare\", \n          \"NoDocbcCost\", \"DiffWalk\", \"Sex\", \"Diabetes_binary\")\ndiabetes_data[cols] &lt;- lapply(diabetes_data[cols], factor)\n\n# Convert Age to factor with meaningful labels\ndiabetes_data$AgeGroup &lt;- factor(diabetes_data$Age,\n                                 levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13),\n                                 labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                                            \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\",\n                                            \"80+\"))\n\ndiabetes_data$Education &lt;- factor(diabetes_data$Education,\n                                  levels = 1:6,\n                                  labels = c(\"Never attended school\", \n                                             \"Elementary\", \n                                             \"Some high school\",\n                                             \"High school graduate\",\n                                             \"Some college\",\n                                             \"College graduate\"))\n\ndiabetes_data$Income &lt;- factor(diabetes_data$Income,\n                               levels = 1:8,\n                               labels = c(\"Less than $10,000\",\n                                          \"$10,000-$15,000\",\n                                          \"$15,000-$20,000\",\n                                          \"$20,000-$25,000\", \n                                          \"$25,000-$35,000\",\n                                          \"$35,000-$50,000\",\n                                          \"$50,000-$75,000\",\n                                          \"$75,000 or more\"))\n                                          \ndiabetes_data$GenHlth &lt;- factor(diabetes_data$GenHlth,\n                                levels = 1:5,\n                                labels = c(\"Excellent\", \"Very good\", \"Good\",\n                                           \"Fair\", \"Poor\"))\n                                             \n\n# Physical activity\nphys_act &lt;- prop.table(table(diabetes_data$PhysActivity, diabetes_data$Diabetes_binary), 2)[2,]\nphys_act\n\n        0         1 \n0.7769427 0.6305381 \n\n# Education and diabetes\nedu_diab_prop &lt;- prop.table(table(diabetes_data$Education, diabetes_data$Diabetes_binary), 1)[,2] \nedu_diab_prop\n\nNever attended school            Elementary      Some high school \n           0.27011494            0.29260450            0.24224520 \n High school graduate          Some college      College graduate \n           0.17635060            0.14810471            0.09690193 \n\n# Income and diabetes\ninc_diab_prop &lt;- prop.table(table(diabetes_data$Income, diabetes_data$Diabetes_binary), 1)[,2]\ninc_diab_prop\n\nLess than $10,000   $10,000-$15,000   $15,000-$20,000   $20,000-$25,000 \n       0.24289063        0.26190274        0.22308366        0.20134095 \n  $25,000-$35,000   $35,000-$50,000   $50,000-$75,000   $75,000 or more \n       0.17401383        0.14507815        0.12182142        0.07960392 \n\n# Check the distribution \ntable(diabetes_data$AgeGroup)\n\n\n18-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 65-69 70-74 75-79   80+ \n 5700  7598 11123 13823 16157 19819 26314 30832 33244 32194 23533 15980 17363 \n\n# Proportion with diabetes in each age group\nage_diab_prop &lt;- prop.table(table(diabetes_data$AgeGroup, diabetes_data$Diabetes_binary), 1)[,2]\nage_diab_prop\n\n     18-24      25-29      30-34      35-39      40-44      45-49      50-54 \n0.01368421 0.01842590 0.02822979 0.04528684 0.06504920 0.08789545 0.11735198 \n     55-59      60-64      65-69      70-74      75-79        80+ \n0.13826544 0.17245217 0.20370255 0.21845918 0.21295369 0.18481829 \n\n\nThese results provide some initial insights into the factors associated with diabetes. For example, we can see that the prevalence of diabetes increases with age and is higher among individuals with lower levels of education and income. To further explore the relationships, we can create visualizations such as bar plots and heatmaps:\n\n# Plot of diabetes prevalence by age group\nbarplot(age_diab_prop, \n        xlab = \"Age Group\", \n        ylab = \"Proportion with Diabetes\", \n        main = \"Diabetes Prevalence by Age Group\",\n        cex.names = 0.7, \n        ylim = c(0,0.5),\n        names.arg = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                      \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\",\n                      \"80+\"),  # Set x-axis labels\n        las = 2, # Rotate x-axis labels\n        col = \"steelblue\" ) #set the color for bars  \n\n\n\n\n\n\n\n\nThe disctribution of the Age group clearly shows that the prevalence of diabetes increases with age.\n\n# Diabetes prevalence by sex\nsex_diab_prop &lt;- prop.table(table(diabetes_data$Sex, diabetes_data$Diabetes_binary), 1)[,2]\nbarplot(sex_diab_prop, \n        xlab = \"Sex\", \n        ylab = \"Proportion with Diabetes\", \n        main = \"Diabetes Prevalence by Sex\",\n        names.arg = c(\"Female\", \"Male\"),\n        col = c(\"pink\", \"lightblue\"),\n        ylim = c(0,0.3))\n\n\n\n\n\n\n\n\nMales have a slightly higher proportion of diabetes compared to females . This suggests that sex may have some influence on diabetes risk, with males being at higher risk.\n\n# Heatmap of diabetes prevalence by age and education\nage_edu_diab_prop &lt;- prop.table(table(diabetes_data$AgeGroup, diabetes_data$Education, diabetes_data$Diabetes_binary), c(1,2))[,,2]\nheatmap(age_edu_diab_prop,\n        xlab = \"Education\",\n        ylab = \"Age Group\",\n        main = \"Diabetes Prevalence by Age and Education\",\n        col = heat.colors(10),\n        cexRow = 0.7, \n        cexCol = 0.7,\n        margins = c(5,5))\n\n\n\n\n\n\n\n\nHeatmap of Diabetes Prevalence by Age and Education:\nThere is a clear trend of increasing diabetes prevalence with age across all education levels. Within each age group, diabetes prevalence is consistently highest among those with the lowest education levels and decreases as education level increases. The highest diabetes prevalence (around 60%) is seen in the oldest age group (80+) with the lowest education level (never attended school).\n\n# BMI and diabetes\nboxplot(BMI ~ Diabetes_binary, \n        data = diabetes_data,\n        xlab = \"Diabetes\", \n        ylab = \"BMI\",\n        main = \"Relationship between BMI and Diabetes\",\n        col = c(\"lightgreen\", \"lightpink\"))\n\n\n\n\n\n\n\n\nRelationship between BMI and Diabetes:\nIndividuals with diabetes have a notably higher median BMI and a larger spread of BMI values compared to those without diabetes. This suggests that higher BMI is strongly associated with increased diabetes risk.\n\n# Income and diabetes\ninc_diab_prop &lt;- prop.table(table(diabetes_data$Income, diabetes_data$Diabetes_binary), 1)[,2]\nbarplot(inc_diab_prop,\n        xlab = \"Income\", \n        ylab = \"Proportion with Diabetes\",\n        main = \"Diabetes Prevalence by Income\",\n        col = \"lightgreen\",\n        las = 2,\n        cex.names = 0.7,\n        ylim = c(0,0.4))\n\n\n\n\n\n\n\n\nDiabetes Prevalence by Income:\nThere is a clear trend of decreasing diabetes prevalence as income increases. The highest diabetes prevalence (around 29%) is seen in the lowest income group ($10,000-$15,000), while the lowest prevalence (around 9%) is in the highest income group ($75,000 or more). This highlights a significant socioeconomic gradient in diabetes risk.\n\n# Physical activity and diabetes\nphys_act_diab_prop &lt;- prop.table(table(diabetes_data$PhysActivity, diabetes_data$Diabetes_binary), 1)[,2]\nbarplot(phys_act_diab_prop,\n        xlab = \"Physically Active\", \n        ylab = \"Proportion with Diabetes\",\n        main = \"Diabetes Prevalence by Physical Activity\",\n        names.arg = c(\"No\", \"Yes\"),\n        col = c(\"lightpink\", \"lightgreen\"),\n        ylim = c(0,0.35))\n\n\n\n\n\n\n\n\nDiabetes Prevalence by Physical Activity:\nIndividuals who are physically inactive have a much higher proportion of diabetes (around 22%) compared to those who are physically active (around 13%). This underscores the importance of physical activity in reducing diabetes risk.\n\n# Diabetes prevalence by health-related factors\nHighBP_summary &lt;- prop.table(table(diabetes_data$Diabetes_binary, diabetes_data$HighBP), margin = 2)\nprint(\"High Blood Pressure Summary:\")\n\n[1] \"High Blood Pressure Summary:\"\n\nprint(HighBP_summary)\n\n   \n             0          1\n  0 0.93964833 0.75554310\n  1 0.06035167 0.24445690\n\n\nHigh Blood Pressure Summary:\nAmong individuals without diabetes (0), 93.96% don’t have high blood pressure (0) and 6.04% have high blood pressure (1). Among individuals with diabetes (1), 75.55% don’t have high blood pressure (0) and 24.45% have high blood pressure (1). The proportion of individuals with high blood pressure is higher among those with diabetes (24.45%) compared to those without diabetes (6.04%). This suggests that having high blood pressure is associated with a higher prevalence of diabetes.\n\nHighChol_summary &lt;- prop.table(table(diabetes_data$Diabetes_binary, diabetes_data$HighChol), margin = 2)\n\nprint(\"High Cholesterol Summary:\")\n\n[1] \"High Cholesterol Summary:\"\n\nprint(HighChol_summary)\n\n   \n             0          1\n  0 0.92018564 0.77985147\n  1 0.07981436 0.22014853\n\n\nAmong individuals without diabetes (0), 92.02% don’t have high cholesterol (0) and 7.98% have high cholesterol (1). Among individuals with diabetes (1), 77.99% don’t have high cholesterol (0) and 22.01% have high cholesterol (1). The proportion of individuals with high cholesterol is substantially higher among those with diabetes (22.01%) compared to those without diabetes (7.98%).\nThis finding suggests that having high cholesterol is strongly associated with a higher prevalence of diabetes. The difference in the proportion of individuals with high cholesterol between the diabetes and non-diabetes groups (22.01% vs 7.98%) is quite large, indicating a significant relationship between these two health conditions.\n\nPhysActivity_summary &lt;- prop.table(table(diabetes_data$Diabetes_binary, diabetes_data$PhysActivity), margin = 2)\n\nprint(\"Physical Activity Summary:\")\n\n[1] \"Physical Activity Summary:\"\n\nprint(PhysActivity_summary)\n\n   \n            0         1\n  0 0.7885525 0.8838735\n  1 0.2114475 0.1161265\n\n\nAmong individuals without diabetes (0), 78.86% are not physically active (0) and 21.14% are physically active (1). Among individuals with diabetes (1), 88.39% are not physically active (0) and 11.61% are physically active (1). The proportion of individuals who are physically active is notably lower among those with diabetes (11.61%) compared to those without diabetes (21.14%).\nThis finding suggests that lack of physical activity is associated with a higher prevalence of diabetes. The difference in the proportion of physically active individuals between the diabetes and non-diabetes groups (11.61% vs 21.14%) is substantial, indicating a significant relationship between physical inactivity and diabetes.\n\nFruits_summary &lt;- prop.table(table(diabetes_data$Diabetes_binary, diabetes_data$Fruits), margin = 2)\n\nprint(\"Fruit Consumption Summary:\")\n\n[1] \"Fruit Consumption Summary:\"\n\nprint(Fruits_summary)\n\n   \n            0         1\n  0 0.8420707 0.8713906\n  1 0.1579293 0.1286094\n\n\nAmong individuals without diabetes (0), 84.21% don’t consume fruit (0) and 15.79% consume fruit (1). Among individuals with diabetes (1), 87.14% don’t consume fruit (0) and 12.86% consume fruit (1). The proportion of individuals who consume fruit is slightly lower among those with diabetes (12.86%) compared to those without diabetes (15.79%).\nThis finding suggests a weak association between not consuming fruit and a higher prevalence of diabetes. The difference in the proportion of fruit consumers between the diabetes and non-diabetes groups (12.86% vs 15.79%) is relatively small, indicating a less substantial relationship compared to physical activity.\n\nVeggies_summary &lt;- prop.table(table(diabetes_data$Diabetes_binary, diabetes_data$Veggies), margin = 2)\n\nprint(\"Vegetable Consumption Summary:\")\n\n[1] \"Vegetable Consumption Summary:\"\n\nprint(Veggies_summary)\n\n   \n            0         1\n  0 0.8200213 0.8701133\n  1 0.1799787 0.1298867\n\n\nAmong individuals without diabetes (0), 82.00% don’t consume vegetables (0) and 18.00% consume vegetables (1). Among individuals with diabetes (1), 87.01% don’t consume vegetables (0) and 12.99% consume vegetables (1). The proportion of individuals who consume vegetables is lower among those with diabetes (12.99%) compared to those without diabetes (18.00%).\nThis finding suggests a weak to moderate association between not consuming vegetables and a higher prevalence of diabetes. The difference in the proportion of vegetable consumers between the diabetes and non-diabetes groups (12.99% vs 18.00%) is somewhat larger than for fruit consumption, but still less substantial than the association seen with physical activity.\n\n\n\nThe analyses reveal several key factors associated with diabetes prevalence in this dataset. Age, education, income, BMI, and physical activity all show significant relationships with diabetes risk. Diabetes prevalence increases markedly with age and is highest among those with the lowest education and income levels. Higher BMI and physical inactivity are also strongly associated with increased diabetes prevalence. Males appear to have a slightly higher risk of diabetes compared to females, though the difference is not as pronounced as for the other factors examined. These findings suggest that efforts to prevent and manage diabetes should focus on promoting healthy lifestyles (maintaining a healthy BMI, engaging in regular physical activity) and addressing socioeconomic disparities in health. Targeted interventions for older adults and those with lower education and income levels may be particularly beneficial in reducing the burden of diabetes in this population.\nThe analysis of these health-related factors reveals several important associations with diabetes prevalence: High blood pressure and high cholesterol show the strongest associations with diabetes. Individuals with these conditions have a much higher prevalence of diabetes compared to those without these conditions. Lack of physical activity is also notably associated with increased diabetes prevalence. Physically inactive individuals are more likely to have diabetes compared to those who are active. Fruit and vegetable consumption show weaker associations with diabetes prevalence, but individuals who don’t consume these foods do have a slightly higher likelihood of having diabetes. These findings suggest that high blood pressure, high cholesterol, and physical inactivity are important risk factors for diabetes in this population. Interventions targeting these factors, such as promoting physical activity and helping individuals manage their blood pressure and cholesterol levels, could potentially help reduce the burden of diabetes. While fruit and vegetable consumption appear to be less strongly associated with diabetes in this analysis, promoting a healthy diet rich in these foods may still be beneficial for overall health and potentially for diabetes prevention and management.\nAmong the three lifestyle factors examined (physical activity, fruit consumption, and vegetable consumption), lack of physical activity shows the strongest association with a higher prevalence of diabetes. The proportion of physically active individuals is markedly lower in the diabetes group compared to the non-diabetes group. Fruit and vegetable consumption show weaker associations with diabetes prevalence. The proportions of individuals consuming these foods are only slightly lower in the diabetes group compared to the non-diabetes group, with vegetable consumption showing a somewhat stronger association than fruit consumption. These findings suggest that promoting physical activity should be a key focus of diabetes prevention and management efforts. While encouraging fruit and vegetable consumption is also important for overall health, these factors appear to have a less direct relationship with diabetes prevalence in this data.\nclick here for the Modeling page"
  },
  {
    "objectID": "EDA.html#the-diabetic-data",
    "href": "EDA.html#the-diabetic-data",
    "title": "Final Project: Diabetic Dataset Analysis",
    "section": "",
    "text": "The Behavioral Risk Factor Surveillance System (BRFSS) is a health-related telephone survey that is collected annually by the CDC. Each year, the survey collects responses from over 400,000 Americans on health-related risk behaviors, chronic health conditions, and the use of preventative services. diabetes _ binary _ health _ indicators _ BRFSS2015.csv is a clean data set of 253,680 survey responses to the CDC’s BRFSS2015. The target variable Diabetes_binary has 2 classes. 0 is for no diabetes, and 1 is for pre-diabetes or diabetes. This data set has 21 feature variables and is not balanced.The ultimate goal is to build a predictive model that can identify individuals at risk of developing diabetes based on the available data.\n\n\nWe perform the exploratory data analysis (EDA) to clean the data and understand the various variables. Check the missing values and investigate if the data is balanced or not. We can run basic summary statistics to get the feel for the data. The purpose of this EDA is to investigate the relationships between various health indicators and the presence of diabetes in the population.\nIn this data set I will be focusing on binary variables (diabetes, sex etc) which have “Yes/No” or male/female values.The target variable is “Diabetes_binary”. The predictor variables are Age, Income, education etc. The meaning full factor levels should be created for ordinal variables (General Health, Education, Age and Income).\n\n# Loading packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(httr)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(dplyr)\nlibrary(tidycensus)\nlibrary(lubridate)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(quantreg)\n\nLoading required package: SparseM\n\n#Read in the data\ndiabetes_data &lt;- read.csv (\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n# Inspect the data\nstr(diabetes_data)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : int  0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP              : int  1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : int  1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : int  1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : int  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : int  1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : int  0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: int  0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : int  0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : int  0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : int  1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : int  1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : int  0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : int  5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : int  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : int  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : int  1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : int  0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : int  9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : int  4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : int  3 1 8 6 4 8 7 4 1 3 ...\n\nsummary(diabetes_data)\n\n Diabetes_binary      HighBP         HighChol        CholCheck     \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :0.0000   Median :0.000   Median :0.0000   Median :1.0000  \n Mean   :0.1393   Mean   :0.429   Mean   :0.4241   Mean   :0.9627  \n 3rd Qu.:0.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n      BMI            Smoker           Stroke        HeartDiseaseorAttack\n Min.   :12.00   Min.   :0.0000   Min.   :0.00000   Min.   :0.00000     \n 1st Qu.:24.00   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000     \n Median :27.00   Median :0.0000   Median :0.00000   Median :0.00000     \n Mean   :28.38   Mean   :0.4432   Mean   :0.04057   Mean   :0.09419     \n 3rd Qu.:31.00   3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:0.00000     \n Max.   :98.00   Max.   :1.0000   Max.   :1.00000   Max.   :1.00000     \n  PhysActivity        Fruits          Veggies       HvyAlcoholConsump\n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   \n Median :1.0000   Median :1.0000   Median :1.0000   Median :0.0000   \n Mean   :0.7565   Mean   :0.6343   Mean   :0.8114   Mean   :0.0562   \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   \n AnyHealthcare     NoDocbcCost         GenHlth         MentHlth     \n Min.   :0.0000   Min.   :0.00000   Min.   :1.000   Min.   : 0.000  \n 1st Qu.:1.0000   1st Qu.:0.00000   1st Qu.:2.000   1st Qu.: 0.000  \n Median :1.0000   Median :0.00000   Median :2.000   Median : 0.000  \n Mean   :0.9511   Mean   :0.08418   Mean   :2.511   Mean   : 3.185  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:3.000   3rd Qu.: 2.000  \n Max.   :1.0000   Max.   :1.00000   Max.   :5.000   Max.   :30.000  \n    PhysHlth         DiffWalk           Sex              Age        \n Min.   : 0.000   Min.   :0.0000   Min.   :0.0000   Min.   : 1.000  \n 1st Qu.: 0.000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 6.000  \n Median : 0.000   Median :0.0000   Median :0.0000   Median : 8.000  \n Mean   : 4.242   Mean   :0.1682   Mean   :0.4403   Mean   : 8.032  \n 3rd Qu.: 3.000   3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:10.000  \n Max.   :30.000   Max.   :1.0000   Max.   :1.0000   Max.   :13.000  \n   Education        Income     \n Min.   :1.00   Min.   :1.000  \n 1st Qu.:4.00   1st Qu.:5.000  \n Median :5.00   Median :7.000  \n Mean   :5.05   Mean   :6.054  \n 3rd Qu.:6.00   3rd Qu.:8.000  \n Max.   :6.00   Max.   :8.000  \n\n# Check for missing values\ncolSums(is.na(diabetes_data))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\nThere are no missing values in the data. First, let’s take a look at the distribution of the target variable, “Diabetes_binary”:\n\ntable(diabetes_data$Diabetes_binary)\n\n\n     0      1 \n218334  35346 \n\nprop.table(table(diabetes_data$Diabetes_binary))\n\n\n       0        1 \n0.860667 0.139333 \n\n\nThis shows that 14% of the individuals in the dataset have diabetes. Next, let’s explore the relationships between the predictor variables and the presence of diabetes. We can start by looking at the prevalence of diabetes across different categories of the predictor variables:\n\n # Convert categorical variables to factors\ncols &lt;- c(\"HighBP\", \"HighChol\", \"CholCheck\", \"Smoker\", \"Stroke\", \"HeartDiseaseorAttack\", \n          \"PhysActivity\", \"Fruits\", \"Veggies\", \"HvyAlcoholConsump\", \"AnyHealthcare\", \n          \"NoDocbcCost\", \"DiffWalk\", \"Sex\", \"Diabetes_binary\")\ndiabetes_data[cols] &lt;- lapply(diabetes_data[cols], factor)\n\n# Convert Age to factor with meaningful labels\ndiabetes_data$AgeGroup &lt;- factor(diabetes_data$Age,\n                                 levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13),\n                                 labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                                            \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\",\n                                            \"80+\"))\n\ndiabetes_data$Education &lt;- factor(diabetes_data$Education,\n                                  levels = 1:6,\n                                  labels = c(\"Never attended school\", \n                                             \"Elementary\", \n                                             \"Some high school\",\n                                             \"High school graduate\",\n                                             \"Some college\",\n                                             \"College graduate\"))\n\ndiabetes_data$Income &lt;- factor(diabetes_data$Income,\n                               levels = 1:8,\n                               labels = c(\"Less than $10,000\",\n                                          \"$10,000-$15,000\",\n                                          \"$15,000-$20,000\",\n                                          \"$20,000-$25,000\", \n                                          \"$25,000-$35,000\",\n                                          \"$35,000-$50,000\",\n                                          \"$50,000-$75,000\",\n                                          \"$75,000 or more\"))\n                                          \ndiabetes_data$GenHlth &lt;- factor(diabetes_data$GenHlth,\n                                levels = 1:5,\n                                labels = c(\"Excellent\", \"Very good\", \"Good\",\n                                           \"Fair\", \"Poor\"))\n                                             \n\n# Physical activity\nphys_act &lt;- prop.table(table(diabetes_data$PhysActivity, diabetes_data$Diabetes_binary), 2)[2,]\nphys_act\n\n        0         1 \n0.7769427 0.6305381 \n\n# Education and diabetes\nedu_diab_prop &lt;- prop.table(table(diabetes_data$Education, diabetes_data$Diabetes_binary), 1)[,2] \nedu_diab_prop\n\nNever attended school            Elementary      Some high school \n           0.27011494            0.29260450            0.24224520 \n High school graduate          Some college      College graduate \n           0.17635060            0.14810471            0.09690193 \n\n# Income and diabetes\ninc_diab_prop &lt;- prop.table(table(diabetes_data$Income, diabetes_data$Diabetes_binary), 1)[,2]\ninc_diab_prop\n\nLess than $10,000   $10,000-$15,000   $15,000-$20,000   $20,000-$25,000 \n       0.24289063        0.26190274        0.22308366        0.20134095 \n  $25,000-$35,000   $35,000-$50,000   $50,000-$75,000   $75,000 or more \n       0.17401383        0.14507815        0.12182142        0.07960392 \n\n# Check the distribution \ntable(diabetes_data$AgeGroup)\n\n\n18-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 65-69 70-74 75-79   80+ \n 5700  7598 11123 13823 16157 19819 26314 30832 33244 32194 23533 15980 17363 \n\n# Proportion with diabetes in each age group\nage_diab_prop &lt;- prop.table(table(diabetes_data$AgeGroup, diabetes_data$Diabetes_binary), 1)[,2]\nage_diab_prop\n\n     18-24      25-29      30-34      35-39      40-44      45-49      50-54 \n0.01368421 0.01842590 0.02822979 0.04528684 0.06504920 0.08789545 0.11735198 \n     55-59      60-64      65-69      70-74      75-79        80+ \n0.13826544 0.17245217 0.20370255 0.21845918 0.21295369 0.18481829 \n\n\nThese results provide some initial insights into the factors associated with diabetes. For example, we can see that the prevalence of diabetes increases with age and is higher among individuals with lower levels of education and income. To further explore the relationships, we can create visualizations such as bar plots and heatmaps:\n\n# Plot of diabetes prevalence by age group\nbarplot(age_diab_prop, \n        xlab = \"Age Group\", \n        ylab = \"Proportion with Diabetes\", \n        main = \"Diabetes Prevalence by Age Group\",\n        cex.names = 0.7, \n        ylim = c(0,0.5),\n        names.arg = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                      \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\",\n                      \"80+\"),  # Set x-axis labels\n        las = 2, # Rotate x-axis labels\n        col = \"steelblue\" ) #set the color for bars  \n\n\n\n\n\n\n\n\nThe disctribution of the Age group clearly shows that the prevalence of diabetes increases with age.\n\n# Diabetes prevalence by sex\nsex_diab_prop &lt;- prop.table(table(diabetes_data$Sex, diabetes_data$Diabetes_binary), 1)[,2]\nbarplot(sex_diab_prop, \n        xlab = \"Sex\", \n        ylab = \"Proportion with Diabetes\", \n        main = \"Diabetes Prevalence by Sex\",\n        names.arg = c(\"Female\", \"Male\"),\n        col = c(\"pink\", \"lightblue\"),\n        ylim = c(0,0.3))\n\n\n\n\n\n\n\n\nMales have a slightly higher proportion of diabetes compared to females . This suggests that sex may have some influence on diabetes risk, with males being at higher risk.\n\n# Heatmap of diabetes prevalence by age and education\nage_edu_diab_prop &lt;- prop.table(table(diabetes_data$AgeGroup, diabetes_data$Education, diabetes_data$Diabetes_binary), c(1,2))[,,2]\nheatmap(age_edu_diab_prop,\n        xlab = \"Education\",\n        ylab = \"Age Group\",\n        main = \"Diabetes Prevalence by Age and Education\",\n        col = heat.colors(10),\n        cexRow = 0.7, \n        cexCol = 0.7,\n        margins = c(5,5))\n\n\n\n\n\n\n\n\nHeatmap of Diabetes Prevalence by Age and Education:\nThere is a clear trend of increasing diabetes prevalence with age across all education levels. Within each age group, diabetes prevalence is consistently highest among those with the lowest education levels and decreases as education level increases. The highest diabetes prevalence (around 60%) is seen in the oldest age group (80+) with the lowest education level (never attended school).\n\n# BMI and diabetes\nboxplot(BMI ~ Diabetes_binary, \n        data = diabetes_data,\n        xlab = \"Diabetes\", \n        ylab = \"BMI\",\n        main = \"Relationship between BMI and Diabetes\",\n        col = c(\"lightgreen\", \"lightpink\"))\n\n\n\n\n\n\n\n\nRelationship between BMI and Diabetes:\nIndividuals with diabetes have a notably higher median BMI and a larger spread of BMI values compared to those without diabetes. This suggests that higher BMI is strongly associated with increased diabetes risk.\n\n# Income and diabetes\ninc_diab_prop &lt;- prop.table(table(diabetes_data$Income, diabetes_data$Diabetes_binary), 1)[,2]\nbarplot(inc_diab_prop,\n        xlab = \"Income\", \n        ylab = \"Proportion with Diabetes\",\n        main = \"Diabetes Prevalence by Income\",\n        col = \"lightgreen\",\n        las = 2,\n        cex.names = 0.7,\n        ylim = c(0,0.4))\n\n\n\n\n\n\n\n\nDiabetes Prevalence by Income:\nThere is a clear trend of decreasing diabetes prevalence as income increases. The highest diabetes prevalence (around 29%) is seen in the lowest income group ($10,000-$15,000), while the lowest prevalence (around 9%) is in the highest income group ($75,000 or more). This highlights a significant socioeconomic gradient in diabetes risk.\n\n# Physical activity and diabetes\nphys_act_diab_prop &lt;- prop.table(table(diabetes_data$PhysActivity, diabetes_data$Diabetes_binary), 1)[,2]\nbarplot(phys_act_diab_prop,\n        xlab = \"Physically Active\", \n        ylab = \"Proportion with Diabetes\",\n        main = \"Diabetes Prevalence by Physical Activity\",\n        names.arg = c(\"No\", \"Yes\"),\n        col = c(\"lightpink\", \"lightgreen\"),\n        ylim = c(0,0.35))\n\n\n\n\n\n\n\n\nDiabetes Prevalence by Physical Activity:\nIndividuals who are physically inactive have a much higher proportion of diabetes (around 22%) compared to those who are physically active (around 13%). This underscores the importance of physical activity in reducing diabetes risk.\n\n# Diabetes prevalence by health-related factors\nHighBP_summary &lt;- prop.table(table(diabetes_data$Diabetes_binary, diabetes_data$HighBP), margin = 2)\nprint(\"High Blood Pressure Summary:\")\n\n[1] \"High Blood Pressure Summary:\"\n\nprint(HighBP_summary)\n\n   \n             0          1\n  0 0.93964833 0.75554310\n  1 0.06035167 0.24445690\n\n\nHigh Blood Pressure Summary:\nAmong individuals without diabetes (0), 93.96% don’t have high blood pressure (0) and 6.04% have high blood pressure (1). Among individuals with diabetes (1), 75.55% don’t have high blood pressure (0) and 24.45% have high blood pressure (1). The proportion of individuals with high blood pressure is higher among those with diabetes (24.45%) compared to those without diabetes (6.04%). This suggests that having high blood pressure is associated with a higher prevalence of diabetes.\n\nHighChol_summary &lt;- prop.table(table(diabetes_data$Diabetes_binary, diabetes_data$HighChol), margin = 2)\n\nprint(\"High Cholesterol Summary:\")\n\n[1] \"High Cholesterol Summary:\"\n\nprint(HighChol_summary)\n\n   \n             0          1\n  0 0.92018564 0.77985147\n  1 0.07981436 0.22014853\n\n\nAmong individuals without diabetes (0), 92.02% don’t have high cholesterol (0) and 7.98% have high cholesterol (1). Among individuals with diabetes (1), 77.99% don’t have high cholesterol (0) and 22.01% have high cholesterol (1). The proportion of individuals with high cholesterol is substantially higher among those with diabetes (22.01%) compared to those without diabetes (7.98%).\nThis finding suggests that having high cholesterol is strongly associated with a higher prevalence of diabetes. The difference in the proportion of individuals with high cholesterol between the diabetes and non-diabetes groups (22.01% vs 7.98%) is quite large, indicating a significant relationship between these two health conditions.\n\nPhysActivity_summary &lt;- prop.table(table(diabetes_data$Diabetes_binary, diabetes_data$PhysActivity), margin = 2)\n\nprint(\"Physical Activity Summary:\")\n\n[1] \"Physical Activity Summary:\"\n\nprint(PhysActivity_summary)\n\n   \n            0         1\n  0 0.7885525 0.8838735\n  1 0.2114475 0.1161265\n\n\nAmong individuals without diabetes (0), 78.86% are not physically active (0) and 21.14% are physically active (1). Among individuals with diabetes (1), 88.39% are not physically active (0) and 11.61% are physically active (1). The proportion of individuals who are physically active is notably lower among those with diabetes (11.61%) compared to those without diabetes (21.14%).\nThis finding suggests that lack of physical activity is associated with a higher prevalence of diabetes. The difference in the proportion of physically active individuals between the diabetes and non-diabetes groups (11.61% vs 21.14%) is substantial, indicating a significant relationship between physical inactivity and diabetes.\n\nFruits_summary &lt;- prop.table(table(diabetes_data$Diabetes_binary, diabetes_data$Fruits), margin = 2)\n\nprint(\"Fruit Consumption Summary:\")\n\n[1] \"Fruit Consumption Summary:\"\n\nprint(Fruits_summary)\n\n   \n            0         1\n  0 0.8420707 0.8713906\n  1 0.1579293 0.1286094\n\n\nAmong individuals without diabetes (0), 84.21% don’t consume fruit (0) and 15.79% consume fruit (1). Among individuals with diabetes (1), 87.14% don’t consume fruit (0) and 12.86% consume fruit (1). The proportion of individuals who consume fruit is slightly lower among those with diabetes (12.86%) compared to those without diabetes (15.79%).\nThis finding suggests a weak association between not consuming fruit and a higher prevalence of diabetes. The difference in the proportion of fruit consumers between the diabetes and non-diabetes groups (12.86% vs 15.79%) is relatively small, indicating a less substantial relationship compared to physical activity.\n\nVeggies_summary &lt;- prop.table(table(diabetes_data$Diabetes_binary, diabetes_data$Veggies), margin = 2)\n\nprint(\"Vegetable Consumption Summary:\")\n\n[1] \"Vegetable Consumption Summary:\"\n\nprint(Veggies_summary)\n\n   \n            0         1\n  0 0.8200213 0.8701133\n  1 0.1799787 0.1298867\n\n\nAmong individuals without diabetes (0), 82.00% don’t consume vegetables (0) and 18.00% consume vegetables (1). Among individuals with diabetes (1), 87.01% don’t consume vegetables (0) and 12.99% consume vegetables (1). The proportion of individuals who consume vegetables is lower among those with diabetes (12.99%) compared to those without diabetes (18.00%).\nThis finding suggests a weak to moderate association between not consuming vegetables and a higher prevalence of diabetes. The difference in the proportion of vegetable consumers between the diabetes and non-diabetes groups (12.99% vs 18.00%) is somewhat larger than for fruit consumption, but still less substantial than the association seen with physical activity.\n\n\n\nThe analyses reveal several key factors associated with diabetes prevalence in this dataset. Age, education, income, BMI, and physical activity all show significant relationships with diabetes risk. Diabetes prevalence increases markedly with age and is highest among those with the lowest education and income levels. Higher BMI and physical inactivity are also strongly associated with increased diabetes prevalence. Males appear to have a slightly higher risk of diabetes compared to females, though the difference is not as pronounced as for the other factors examined. These findings suggest that efforts to prevent and manage diabetes should focus on promoting healthy lifestyles (maintaining a healthy BMI, engaging in regular physical activity) and addressing socioeconomic disparities in health. Targeted interventions for older adults and those with lower education and income levels may be particularly beneficial in reducing the burden of diabetes in this population.\nThe analysis of these health-related factors reveals several important associations with diabetes prevalence: High blood pressure and high cholesterol show the strongest associations with diabetes. Individuals with these conditions have a much higher prevalence of diabetes compared to those without these conditions. Lack of physical activity is also notably associated with increased diabetes prevalence. Physically inactive individuals are more likely to have diabetes compared to those who are active. Fruit and vegetable consumption show weaker associations with diabetes prevalence, but individuals who don’t consume these foods do have a slightly higher likelihood of having diabetes. These findings suggest that high blood pressure, high cholesterol, and physical inactivity are important risk factors for diabetes in this population. Interventions targeting these factors, such as promoting physical activity and helping individuals manage their blood pressure and cholesterol levels, could potentially help reduce the burden of diabetes. While fruit and vegetable consumption appear to be less strongly associated with diabetes in this analysis, promoting a healthy diet rich in these foods may still be beneficial for overall health and potentially for diabetes prevention and management.\nAmong the three lifestyle factors examined (physical activity, fruit consumption, and vegetable consumption), lack of physical activity shows the strongest association with a higher prevalence of diabetes. The proportion of physically active individuals is markedly lower in the diabetes group compared to the non-diabetes group. Fruit and vegetable consumption show weaker associations with diabetes prevalence. The proportions of individuals consuming these foods are only slightly lower in the diabetes group compared to the non-diabetes group, with vegetable consumption showing a somewhat stronger association than fruit consumption. These findings suggest that promoting physical activity should be a key focus of diabetes prevention and management efforts. While encouraging fruit and vegetable consumption is also important for overall health, these factors appear to have a less direct relationship with diabetes prevalence in this data.\nclick here for the Modeling page"
  }
]